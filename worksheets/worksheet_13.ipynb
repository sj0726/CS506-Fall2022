{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 13\n",
    "\n",
    "Name:  Sangjoon Lee\n",
    "UID: U79516048\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Support Vector Machines\n",
    "- Recommender Systems\n",
    "\n",
    "## Support Vector Machines\n",
    "\n",
    "Follow along in class to implement the perceptron algorithm and create an animation of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "CENTERS = [[0, 1], [1, 0]]\n",
    "\n",
    "# Dataset\n",
    "X, labels = datasets.make_blobs(n_samples=10, centers=CENTERS, cluster_std=0.2, random_state=0)\n",
    "Y = np.array(list(map(lambda x : -1 if x == 0 else 1, labels.tolist())))\n",
    "\n",
    "# Initializing w and b\n",
    "w = np.array([1, 1])\n",
    "b = 0\n",
    "\n",
    "# Perceptron Parameters\n",
    "epochs = 100\n",
    "alpha = .05\n",
    "expanding_rate = .99\n",
    "retracting_rate = 1.1\n",
    "\n",
    "def snap(x, w, b, error):\n",
    "    \"\"\"\n",
    "        Plot the street induced by w and b.\n",
    "        Circle the point x in red if it was\n",
    "        misclassified or in yellow if it was\n",
    "        classified correctly.\n",
    "    \"\"\"\n",
    "\n",
    "    xplot = np.linspace(-3, 3)\n",
    "    cs = np.array([x for x in 'gb'])\n",
    "\n",
    "    svm = - (w[0] / w[1]) * xplot - (b / w[1])\n",
    "    left_svm = - (w[0] / w[1]) * xplot - (b / w[1]) - 1 / w[1]\n",
    "    right_svm = - (w[0] / w[1]) * xplot - (b / w[1]) + 1 / w[1]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0],X[:,1],color=cs[labels].tolist(), s=50, alpha=0.8)\n",
    "    if error:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .2, color='r',fill=False))\n",
    "    else:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .2, color='y',fill=False))\n",
    "    ax.plot(xplot, left_svm, 'r--', lw=2)\n",
    "    ax.plot(xplot, svm, 'r-', lw=2)\n",
    "    ax.plot(xplot, right_svm, 'r--', lw=2)\n",
    "    ax.set_xlim(min(X[:, 0]) - 1, max(X[:,0]) + 1)\n",
    "    ax.set_ylim(min(X[:, 1]) - 1, max(X[:,1]) + 1)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "def accuracy(w, b):\n",
    "    acc = 0\n",
    "    for x in zip(X, Y):\n",
    "        ypred = np.dot(w, x) + b\n",
    "        if (ypred >= 0 and y > 0) or (ypred < 0 and y < 0):\n",
    "            acc += 1\n",
    "    return acc / len(X)\n",
    "\n",
    "images = []\n",
    "for _ in range(epochs):\n",
    "    # pick a point from X at random\n",
    "    i = np.random.randint(0, len(X))\n",
    "    x, y = X[i], Y[i]\n",
    "    error = False\n",
    "    ypred = np.dot(w, x) + b\n",
    "    if (ypred >= 0 and y > 0) or (ypred < 0 and y < 0):\n",
    "        # correctly classified\n",
    "        if ypred <= 1 and ypred >= -1:\n",
    "            # but the point is in the street\n",
    "            w = w + alpha * y * x * retracting_rate\n",
    "            b += alpha * y * retracting_rate\n",
    "        else:\n",
    "            # the point is in the margin\n",
    "            w = w * expanding_rate\n",
    "            b *= expanding_rate\n",
    "    else:\n",
    "        # misclassified\n",
    "        w = w + alpha * y * x * expanding_rate\n",
    "        b += alpha * y * expanding_rate\n",
    "        \n",
    "    images.append(snap(x, w, b, error))\n",
    "    # print(\"training accuracy = \", accuracy(w, b))\n",
    "\n",
    "images[0].save(\n",
    "    'svm.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in class, the form\n",
    "$$w^T x + b = 0$$\n",
    "while simple, does not expose the inner product `<x_i, x_j>` which we know `w` depends on, having done the math. This is critical to applying the \"kernel trick\" which allows for learning non-linear decision boundaries. Let's modify the above algorithm to use the form\n",
    "$$\\sum_i \\alpha_i <x_i, x> + b = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "CENTERS = [[0, 1], [1, 0]]\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = .05\n",
    "expanding_rate = .99\n",
    "retracting_rate = 1.1\n",
    "\n",
    "X, labels = datasets.make_blobs(n_samples=10, centers=CENTERS, cluster_std=0.2, random_state=0)\n",
    "Y = np.array(list(map(lambda x : -1 if x == 0 else 1, labels.tolist())))\n",
    "\n",
    "alpha_i = np.zeros((len(X),))\n",
    "b = 0\n",
    "\n",
    "def snap(x, alpha_i, b, error):\n",
    "    # create a mesh to plot in\n",
    "    h = .01  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    meshData = np.c_[xx.ravel(), yy.ravel()]\n",
    "    cs = np.array([x for x in 'gb'])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0],X[:,1],color=cs[labels].tolist(), s=50, alpha=0.8)\n",
    "\n",
    "    if error:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='r',fill=False))\n",
    "    else:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='y',fill=False))\n",
    "   \n",
    "    Z = predict_many(alpha_i, b, meshData)\n",
    "    Z = np.array([0 if z <=0 else 1 for z in Z]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=.5, cmap=plt.cm.Paired)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "def predict_many(alpha_i, b, Z):\n",
    "    res = []\n",
    "    for i in range(len(Z)):\n",
    "        res.append(predict(alpha_i, b, Z[i]))\n",
    "    return np.array(res)\n",
    "\n",
    "def predict(alpha_i, b, x):\n",
    "    wx = 0\n",
    "    for i in range(len(X)):\n",
    "        wx += alpha_i[i] * polynomial(X[i], x, 0, 2)\n",
    "    return wx + b\n",
    "\n",
    "def accuracy(w, b):\n",
    "    acc = 0\n",
    "    for x in zip(X, y):\n",
    "        ypred = np.dot(w, x) + b\n",
    "        if (ypred >= 0 and y > 0) or (ypred < 0 and y < 0):\n",
    "            acc += 1\n",
    "    return acc / len(X)\n",
    "\n",
    "images = []\n",
    "for _ in range(epochs):\n",
    "    # pick a point from X at random\n",
    "    i = np.random.randint(0, len(X))\n",
    "    error = False\n",
    "    x, y = X[i], Y[i]\n",
    "    ypred = predict(alpha_i, b, x)\n",
    "    if (ypred >= 0 and y > 0) or (ypred < 0 and y < 0):\n",
    "        # correctly classified\n",
    "        if ypred <= 1 and ypred >= -1:\n",
    "            # but the point is in the street\n",
    "            alpha_i[i] += learning_rate * y\n",
    "            b += learning_rate * y * retracting_rate\n",
    "            alpha_i = alpha_i * retracting_rate\n",
    "        else:\n",
    "            # the point is in the margin\n",
    "            alpha_i = alpha_i * expanding_rate\n",
    "            b *= expanding_rate\n",
    "    else:\n",
    "        # misclassified\n",
    "        alpha_i[i] += learning_rate * y\n",
    "        b += learning_rate * y * expanding_rate\n",
    "        alpha_i = alpha_i * expanding_rate\n",
    "\n",
    "    images.append(snap(x, alpha_i, b, error))\n",
    "\n",
    "images[0].save(\n",
    "    'svm_dual.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a configurable kernel function to apply in lieu of the dot product. Try it out on a dataset that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x_i, x_j, c, n):\n",
    "    return (np.dot(x_i, x_j) + c) ** n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "In the example in class of recommending movies to users we used the movie rating as a measure of similarity between users and movies and thus the predicted rating for a user is a proxy for how highly a movie should be recommended. So the higher the predicted rating for a user, the higher a recommendation it would be.\n",
    "\n",
    "a) Consider a streaming platform that only has \"like\" or \"dislike\" (no 1-5 rating). Describe how you would build a recommender system in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's only true/false value here, the recommender system should be simplified to recommending a movie if a user has enough likes in certain types of movies, rather than a threshold of values for whether it should be recommended it or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Describe 3 challenges of building a recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lack of data: The system becomes effective with lots of data to begin with. If there's not enough data to be trained with, the accuracy of the recommender system will be low.\n",
    "\n",
    "Changing data/user preferences: The trends are always changing, and so are the user preferences, so it's hard to be accurate with recommendations when the user's preference or general trend shifts in a direction that the system is not trained upon.\n",
    "\n",
    "Unpredicatability: There may be some items where there's a distinct division between those who like/prefer the item and those who don't, regardless of their general user trend. These items will be hard to recommend for the system as these count as outliers in terms of data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Why is SVD not an option for collaborative filtering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is little to no explanation as to why an item is recommended to an user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
